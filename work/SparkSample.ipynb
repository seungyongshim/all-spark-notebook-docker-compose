{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae3a24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://533e692306bf:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1619598751488)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [registration_dttm: timestamp, id: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.parquet(\"../data/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c723add",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- registration_dttm: timestamp (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- cc: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- birthdate: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- comments: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad6765de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SaveMode\n",
       "import java.util.Properties\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import java.util.Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52046fda",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " Column \"registration_dttm\" not found in schema Some(StructType(StructField(id,IntegerType,true)))",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: Column \"registration_dttm\" not found in schema Some(StructType(StructField(id,IntegerType,true)))",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$4(JdbcUtils.scala:143)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$getInsertStatement$2(JdbcUtils.scala:143)",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)",
      "  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)",
      "  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:238)",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:141)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:846)",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)",
      "  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)",
      "  at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:817)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "val properties = new Properties()\n",
    "properties.put(\"user\", \"dev_seoungyongshim\")\n",
    "properties.put(\"password\", \"Seoungyongshim@5678roqkf\")\n",
    "df.write.mode(SaveMode.Append).jdbc(\"jdbc:mysql://10.248.193.229/midash\", \"Test\", properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ec8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
